# Questions from week 4
## Perceptrons
- What is a perceptron?
- Under what conditions is a perceptron guaranteed to converge?
- Why was the perceptron unable to solve the XOR problem?
- What is the nonlinear activation function used in a perceptron?
- Why does a multilayer perceptron (MLP) solve the XOR problem?
- What was the key insight that allowed training of MLP-like networks?

## Backpropagation
- What is being calculated in backpropagation?
- What are the forward and backward passes?
- How are backpropagation and gradient descent related?
- What are some computational efficiencies that are commonly used to speed up backpropagation?
- Why are bias terms necessary in a neural network?
- Why are nonlinear activation functions necessary in a neural network?
- What is the difference between hidden and output activation functions?
- Why is ReLU (and its variations) more popular than the sigmoid function for hidden layers?
- What drives the choice of loss and output activation functions?
- What are the parameters in a network?