{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring the multi-layer perceptron\n",
    "In the demo code from lecture, we implemented a multi-layer perceptron (MLP) with one hidden layer, but it's written in a very brittle fashion with everything being just one loop.\n",
    "\n",
    "For now, we'll stick to the simple model without activation functions (so it's still just linear transformations all the way), but the refactoring will make it easier to add activation functions later.\n",
    "\n",
    "## Objectives\n",
    "- Refactor the code to make it more modular\n",
    "- Understand the steps in the training loop\n",
    "- Visualize the learned hidden space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, here's the original training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy mlp example\n",
    "# forward pass\n",
    "x = np.array([2, 3])\n",
    "y = 1\n",
    "w1 = np.array([[-0.78, 0.13], [0.85, 0.23]])\n",
    "w2 = np.array([1.8, 0.40])\n",
    "\n",
    "iterations = 20\n",
    "eta = 0.01\n",
    "loss = np.zeros(iterations)\n",
    "\n",
    "for i in range(iterations):\n",
    "    # forward pass\n",
    "    y_hat = x @ w1 @ w2\n",
    "    \n",
    "    # update loss\n",
    "    loss[i] = 0.5 * (y_hat - y)**2\n",
    "\n",
    "    # backpropagate!\n",
    "    w2_partials = (y_hat - y) * (x @ w1)\n",
    "    w1_partials = np.outer(w2_partials, x)\n",
    "\n",
    "    w1 = w1 - eta * w1_partials\n",
    "    w2 = w2 - eta * w2_partials\n",
    "\n",
    "plt.plot(loss)\n",
    "\n",
    "# check how well we did\n",
    "print(\"Final prediction:\", x @ w1 @ w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a set of functions to perform the actions of the MLP:\n",
    "\n",
    "- `forward`: forward pass through the network\n",
    "- `loss`: calculate the loss\n",
    "- `backward`: backward pass (backpropagation) through the network, calculating the gradients\n",
    "- `train`: update the weights of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, w1, w2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, string the various functions together in a new training loop. The results should be the same as in the original, but now we have a more modular implementation.\n",
    "\n",
    "## Part 2: Visualizing the hidden space\n",
    "As we saw in the lecture, it's impossible to draw a line through the scatter plot of $x_1$ and $x_2$ in the XOR case. However, the output neuron is still just a linear transformation, so it must be possible to draw a line through the hidden space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumes binary classification\n",
    "def plot_data(x, y):\n",
    "    y_false = y.flatten() == 0\n",
    "    y_true = y.flatten() == 1\n",
    "    plt.scatter(x[y_false,0],x[y_false,1],color=\"blue\", marker=\"o\", label=\"Output: 0\")\n",
    "    plt.scatter(x[y_true,0],x[y_true,1],color=\"red\", marker=\"x\", label=\"Output: 1\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Input 1\")\n",
    "    plt.ylabel(\"Input 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR gate\n",
    "x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,1,1,0])\n",
    "plot_data(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP with hard-coded weights to solve XOR\n",
    "X = np.hstack((np.ones((x.shape[0],1)),x))\n",
    "\n",
    "w1 = np.array([[0, 1, 1], [-1, 1, 1]]).T\n",
    "\n",
    "H = (X @ w1 > 0).astype(int)\n",
    "H = np.hstack((np.ones((H.shape[0],1)),H))\n",
    "\n",
    "w2 = np.array([0, 1, -1])\n",
    "y = (H @ w2 > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, write the code to plot the hidden space. You should see that it's possible to draw a line through the hidden space that separates the two classes. This is a single line of code, but it does require understanding what each variable represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercise\n",
    "Try extending your refactored MLP to handle a whole design matrix instead of a single sample. This will require some numpy magic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
