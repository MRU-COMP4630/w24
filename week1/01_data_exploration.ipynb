{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end Machine Learning project\n",
    "\n",
    "Based on the book \"Hands-On Machine Learning with Scikit-Learn and TensorFlow\" by Aur√©lien G√©ron\n",
    "\n",
    "Adapted from original [notebook](https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb) by Aur√©lien G√©ron \n",
    "This notebook requires Python 3.7 or later and scikit-learn 1.0.1 or later.\n",
    "\n",
    "I highly recommend using [Colab](https://colab.research.google.com/) or a [virtual environment](https://docs.python.org/3/tutorial/venv.html) to keep all your dependencies contained and frozen to a specific version. Note that tensorflow (to be used later in the course) only supports up to Python 3.11 right now, and only supports GPUs on Linux (including WSL2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "import sklearn\n",
    "\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level Machine Learning Project Checklist\n",
    "Appendix B (2019) or Appendix A (2022)\n",
    "\n",
    "1. Frame the problem and look at the big picture.\n",
    "2. Get the data.\n",
    "3. Explore the data to gain insights.\n",
    "4. Prepare the data to better expose the underlying data patterns to Machine Learning algorithms.\n",
    "5. Explore many different models and short-list the best ones.\n",
    "6. Fine-tune your models and combine them into a great solution.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system.\n",
    "\n",
    "## Frame the problem and look at the big picture\n",
    "*Welcome to Machine Learning Housing Corp.! Your task is to predict median house values in Californian districts, given a number of features from these districts.*\n",
    "\n",
    "**How does the company expect to use and benefit from this model?**\n",
    "The model‚Äôs output, along with many other signals, will be used to determine whether it is worth investing in a given area or not.\n",
    "\n",
    "**What the current solution looks like (if any).** The current situation will often give you a reference for performance, as well as insights on how to solve the problem. \n",
    "\n",
    "Answer: The district housing prices are currently estimated manually by experts: a team gathers up-to-date information about a district, and when they cannot get the median housing price, they estimate it using complex rules.\n",
    "This is costly and time-consuming, and their estimates are not great; in cases where they manage to find out the actual median housing price, they often realize that their estimates were off by more than 20%.\n",
    "\n",
    "**‚ùì Discussions for class:**\n",
    "- What kind of ML task is this?\n",
    "- What kind of performance measure should we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def load_housing_data():\n",
    "    dir_path = Path(\"../datasets\")\n",
    "    tarball_path = dir_path / \"housing.tgz\"\n",
    "    if not tarball_path.is_file():\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path=dir_path)\n",
    "    return pd.read_csv(dir_path / \"housing\" / \"housing.csv\")\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a Quick Look at the Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first few rows of the housing dataframe\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the data\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the categories in the ocean_proximity column\n",
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot a histogram for each numerical attribute\n",
    "housing.hist(bins=50, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ‚ùì Discussions for class:\n",
    "- What do you notice about the data?\n",
    "- Do the values make sense for the labels?\n",
    "- Is the scale of the features comparable? Does this matter?\n",
    "- What possible biases might be present in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Test Set\n",
    "\n",
    "**‚ùì Discussions for class:**\n",
    "- Why do we need a test set?\n",
    "- What is data snooping bias?\n",
    "- How should we create the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive approach: use the index as the identifier and randomly select 20%\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "def shuffle_and_split_data(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = shuffle_and_split_data(housing, 0.2)\n",
    "print(\"Training samples: \", len(train_set), \"Testing samples: \", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì Discussions for class:\n",
    "- What is the purpose of the random seed?\n",
    "- What is naive about this approach? (Hint: what if the dataset is updated?)\n",
    "- What alternative identifier could we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash-based identifier\n",
    "Instead of randomly permuting the indices, we can compute a hash of each instance's identifier and select samples that are less than 20% of the maximum hash value. This ensures that the test set will remain consistent across multiple runs, even if we refresh the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "def is_id_in_test_set(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) < test_ratio * 2**32\n",
    "\n",
    "def split_data_with_id_hash(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_with_id = housing.reset_index()  # adds an `index` column\n",
    "train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this dataset doesn't have a unique identifier other than the row number, which doesn't protect against insertions in the dataset. Another solution is to pick something that uniquely identifies the sample, such as a combination of the district's latitude and longitude (although this isn't perfect either, as some districts may be close enough together that their ids are computed to be the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
    "train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn's train_test_split function does basically the same thing as shuffle_and_split_data, with some added magic\n",
    "# This function is commonly used but it's important to understand its assumptions and limitations!\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling bias\n",
    "Random sampling is fine if the dataset is large enough relative to the number of attributes, but if it is not, you run the risk of introducing **sampling bias**.\n",
    "\n",
    "**üßÆ Math break!**\n",
    "\n",
    "Say we want to make sure that survey respondents represent the population ratio of 51.1% female and 48.9% male ($\\pm 3\\%$). The [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution) can be used to model the probability of choosing $k$ female participants from $n$ total participants:\n",
    "\n",
    "$$P(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$$\n",
    "\n",
    "$P(X = k)$ is the probability mass function, and the corresponding cumulative distribution function is just the sum up to $k$:\n",
    "\n",
    "$$P(X \\leq k) = \\sum_{i=0}^k \\binom{n}{i}p^i(1-p)^{n-i}$$\n",
    "\n",
    "You can see how this depends on $n$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "p = 0.511 # ratio of female to male\n",
    "buffer = 0.03\n",
    "sample_sizes = [10, 100, 500, 1000, 5000, 10000]\n",
    "prob_bias = []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    too_small = n * (p - buffer)\n",
    "    too_large = n * (p + buffer)\n",
    "    proba_too_small = binom(n, p).cdf(too_small - 1)\n",
    "    proba_too_large = 1 - binom(n, p).cdf(too_large)\n",
    "    prob_bias.append((proba_too_small + proba_too_large) * 100)\n",
    "\n",
    "print(sample_sizes)\n",
    "print(prob_bias)\n",
    "plt.plot(sample_sizes, prob_bias, \"o-\")\n",
    "plt.xlabel(\"Sample size\")\n",
    "plt.ylabel(\"Probability of sampling bias (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified sampling\n",
    "Instead of taking a naive random sample, we can use **stratified sampling** to ensure that the test set is representative of the overall population. The population is divided into smaller subgroups called **strata**, and a representative random sample is drawn from each.\n",
    "\n",
    "Scikit-learn provides a handy option for this in `train_test_split`, but we need to decide what the strata should be. Here's where **domain knowledge** comes in - in this case, let's say we were told that the current manual process uses median income as a proxy for median housing price, so we should make sure that the test set is representative of the income distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see that it's a reasonable proxy\n",
    "plt.scatter(housing[\"median_income\"], housing[\"median_house_value\"], alpha=0.1)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Median income\")\n",
    "plt.ylabel(\"Median house value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some kind of odd stuff going on in this plot, like the obvious ceiling at 500k. There's some weaker lines at 450k and 350k as well. Eventually we might want to deal with these outliers, but for now we'll ignore them.\n",
    "\n",
    "### ‚ùì Discussions for class:\n",
    "- How might we deal with the outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the median income into reasonable categories in order to create strata buckets\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "housing[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the sampling by looking at the histogram of the test set\n",
    "strat_test_set[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't actually want the income_cat column sticking around, so let's drop it\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover and Visualize the Data to Gain Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the training set to mess around with\n",
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Geographical Data\n",
    "\n",
    "Not every data set makes sense to plot on a map, but in this case we have latitude and longitude. Might as well plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n",
    "             s=housing[\"population\"] / 100, label=\"population\",\n",
    "             c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n",
    "             legend=True, figsize=(10, 7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for Correlations\n",
    "\n",
    "**üßÆ Math break!**\n",
    "\n",
    "The [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is a measure of the linear correlation between two variables $X$ and $Y$ (commonly denoted as $r$):\n",
    "\n",
    "$$r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}$$\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the sample means of $X$ and $Y$, respectively.\n",
    "\n",
    "Coefficients close to 1 indicate strong positive correlation, and coefficients close to -1 indicate strong negative correlation.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg)\n",
    "\n",
    "We'll revisit the concept of correlation later on when we talk about convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr(numeric_only=True)\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas' scatter_matrix function plots every numerical attribute against every other numerical attribute\n",
    "# This can be useful, but you probably want to restrict the number of attributes to plot\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Attribute Combinations\n",
    "\n",
    "Aka **feature engineering**. This is another place where domain knowledge is important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
    "housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
    "housing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sorted = corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "corr_sorted.plot(kind=\"bar\")\n",
    "plt.plot([-0.5, len(corr_sorted)], [0, 0], \"k--\", linewidth=2)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Attribute\")\n",
    "plt.ylabel(\"Correlation Coefficient\")\n",
    "plt.title(\"Correlation of Attributes with Median House Value\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
