{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 of the end-to-end example\n",
    "\n",
    "In the last notebook, we explored the data and did some feature engineering. In this notebook, we'll apply the basic processing steps from the previous one, then train and fine-tune a model.\n",
    "\n",
    "Ultimately this repeated code would probably be better as a separate script, but I'm being lazy and copy pasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def load_housing_data():\n",
    "    dir_path = Path(\"../datasets\")\n",
    "    tarball_path = dir_path / \"housing.tgz\"\n",
    "    if not tarball_path.is_file():\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path=dir_path)\n",
    "    return pd.read_csv(dir_path / \"housing\" / \"housing.csv\")\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified sampling\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Add the income category and do our stratified sampling\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)\n",
    "\n",
    "# We don't actually want the income_cat column sticking around, so let's drop it\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do our feature engineering\n",
    "housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
    "housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
    "housing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "We don't want the target (the median house value) to be in the predictors, so we'll split the training set into predictor and target (often called $x$ and $y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "### Missing features\n",
    "Most ML algorithms can't deal with missing features, so we need to do so during the wrangling step.\n",
    "\n",
    "In the book 3 options are listed to handle the NaN values:\n",
    "\n",
    "```python\n",
    "housing.dropna(subset=[\"total_bedrooms\"], inplace=True)    # option 1\n",
    "\n",
    "housing.drop(\"total_bedrooms\", axis=1)       # option 2\n",
    "\n",
    "median = housing[\"total_bedrooms\"].median()  # option 3\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True)\n",
    "```\n",
    "\n",
    "For each option, we'll create a copy of `housing` and work on that copy to avoid breaking `housing`. We'll also show the output of each option, but filtering on the rows that originally contained a NaN value.\n",
    "\n",
    "**❓ Discussions for class:**\n",
    "- What is each option doing?\n",
    "- What are the pros and cons of each option?\n",
    "- Which one should we choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's look at the missing values in the total_bedrooms column\n",
    "null_rows_idx = housing.isnull().any(axis=1)\n",
    "print(f\"Missing {len(housing[null_rows_idx])} of {len(housing)} rows\")\n",
    "housing.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing the NaN values with the median is surprisingly common, as it preserves the most data. Scikit-learn provides a `SimpleImputer` class to do this automatically. There's also `KNNImputer` which uses the $k$ nearest neighbors to fill in the missing values, or `IterativeImputer` which repeatedly trains a regression model to predict the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't work on text data, so we need to filter the dataframe to just include numeric types. After that, we can use the imputer as a **transformer** to fill in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing.select_dtypes(include=[np.number])\n",
    "imputer.fit(housing_num)\n",
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index=housing_num.index)\n",
    "housing_tr.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"pandas\")  # scikit-learn >= 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "As we saw on the various scatter plots, there's some outliers in the data. One way to detect them is to use an [isolation forest](https://doi.org/10.1109/ICDM.2008.17), which recursively splits the data into subsets until each contains only one sample. Outliers are samples that require fewer splits to isolate. The `IsolationForest` class in scikit-learn implements this algorithm and can be used to train a model to predict an anomaly score for each sample, with -1 being an outlier and 1 being an inlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "isolation_forest = IsolationForest(random_state=42)\n",
    "outlier_pred = isolation_forest.fit_predict(X)\n",
    "outlier_pred[outlier_pred == -1].size / outlier_pred.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to drop outliers, you would run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#housing = housing.iloc[outlier_pred == 1]\n",
    "#housing_labels = housing_labels.iloc[outlier_pred == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Text and Categorical Attributes\n",
    "\n",
    "Most of the math in ML algorithms is based on numbers, so we need to convert text and categorical attributes to numbers. This is called **encoding**.\n",
    "\n",
    "**❓ Discussions for class:**\n",
    "- Which columns of our data are categorical?\n",
    "- What methods could we use to convert them to numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "\n",
    "plt.hist(housing_cat_encoded)\n",
    "plt.xticks(range(5), ordinal_encoder.categories_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**❓ Discussions for class:**\n",
    "- What does the `OrdinalEncoder` assume?\n",
    "- Does it make sense in this context?\n",
    "\n",
    "An alternative method is a **One-hot encoder**, which creates a new binary attribute for each category. This can get really big if there are a lot of categories, but it's fine for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder(sparse_output=False) # pandas doesn't like spare matrices\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "The last piece of data cleaning that we want to look at is feature scaling. Most ML algorithms don't like hugely different scales, so we want to scale the features to be similar. Two popular methods are **min-max scaling** and **standardization**:\n",
    "\n",
    "- Min-max scaling (also called normalization) shifts and rescales the data so that it ends up ranging from 0 to 1.\n",
    "- Standardization subtracts the mean value and divides by the standard deviation so that the resulting distribution has unit variance (aka the **standard normal distribution**).\n",
    "\n",
    "**🧮 Math break!**\n",
    "\n",
    "A general Gaussian distribution is given by:\n",
    "\n",
    "$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left(\\frac{x - \\mu}{\\sigma}\\right)^2} $$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation. The standard normal distribution is a special case where $\\mu = 0$ and $\\sigma = 1$, reducing the equation to:\n",
    "\n",
    "$$ f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} x^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mu, sigma):\n",
    "    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2)) / np.sqrt(2 * np.pi * sigma ** 2)\n",
    "\n",
    "x = np.linspace(-5, 7, 100)\n",
    "plt.plot(x, gaussian(x, 0, 1), label=\"$\\mu=0$, $\\sigma=1$\")\n",
    "plt.plot(x, gaussian(x, 1, 2), label=\"$\\mu=1$, $\\sigma=2$\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The normal distribution is important because of the **central limit theorem**, which says that the **sum** of a large number of **independent and identically distributed** (i.i.d) random variables is approximately normally distributed. For example, consider the simplest possible random variable: the Bernoulli distribution, which has two possible outcomes (0 or 1) with probabilities $p$ and $1-p$ respectively.\n",
    "\n",
    "In the previous notebook, we saw that the average of $n$ Bernoulli random variables is described by the binomial distribution:\n",
    "\n",
    "$$ f(k;n,p) = \\Pr(k;n,p) = \\Pr(X = k) = \\binom{n}{k} p^k (1-p)^{n-k} $$\n",
    "\n",
    "where $k$ is the number of successes, $n$ is the number of trials, and $p$ is the probability of success. The **mean** or **expected value** of the binomial distribution is $np$ and the **variance** is $np(1-p)$.\n",
    "\n",
    "The central limit theorem says that if we take the average of $n$ Bernoulli random variables, the distribution of the average will be approximately normal, with the same mean and standard deviation.\n",
    "\n",
    "> Important! This works for the binomial distribution because it is already the **sum** of i.i.d. random variables. The CLT does **not** say that as $n$ gets large, the distribution of **any** random variable will be approximately normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom, norm\n",
    "\n",
    "p = 0.3 # let's say the average student has a 30% chance of falling asleep in class\n",
    "n = 6 # there's only 6 students in the class\n",
    "\n",
    "x = np.arange(-1, n + 1)\n",
    "plt.bar(x, binom.pmf(x, n, p), label=\"Binomial pmf\")\n",
    "plt.plot(x, norm.pdf(x, n * p, np.sqrt(n * p * (1 - p))), color=\"red\", label=\"Normal approximation\")\n",
    "plt.xlabel(\"Number of students asleep\")\n",
    "plt.ylabel(\"Probability\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the task at hand... here's how you would normalize the data using scikit-learn's `MinMaxScaler`, or standardize it using `StandardScaler`. First, let's recall the distributions of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.hist(bins=50, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)\n",
    "housing_num_min_max_scaled.hist(bins=50, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)\n",
    "housing_num_std_scaled.hist(bins=50, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**❓ Discussions for class:**\n",
    "- What are the bounds of each method?\n",
    "- Which method is more affected by outliers?\n",
    "- How would you decide which method to use?\n",
    "\n",
    "### Other transformations\n",
    "Sometimes you might want to do things like log-transform the data to deal with long tails (skewness), or even convert a numerical attribute to a categorical one to deal with a multi-modal distribution (like `latitude` above). The book goes into details on a few more examples, but the main takeaway is that there is no rule that says you need to feed your data as-is into an ML algorithm, and data transformations might make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, with_mean=True):  # no *args or **kwargs!\n",
    "        self.with_mean = with_mean\n",
    "\n",
    "    def fit(self, X, y=None):  # y is required even though we don't use it\n",
    "        X = check_array(X)  # checks that X is an array with finite float values\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        self.scale_ = X.std(axis=0)\n",
    "        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)  # looks for learned attributes (with trailing _)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1]\n",
    "        if self.with_mean:\n",
    "            X = X - self.mean_\n",
    "        return X / self.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformations are so common that most frameworks have a way of handling them efficiently. We'll set up a pipeline of functional transformations that will be applied to the data in sequence. A key thing to note is that the pipeline is fit to the **training data** - you should never re-normalize based on test data, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "# let's go with the median imputer to fill in the missing total_bedrooms value\n",
    "# Use the standard scaler for normalization\n",
    "num_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"standardize\", StandardScaler()),\n",
    "])\n",
    "\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the book and original version of this notebook include some fancier methods of\n",
    "# composing pipelines. I've removed them as it's all just syntax.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
    "               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the transform to the housing data, which is just the training set\n",
    "housing_prepared = preprocessing.fit_transform(housing)\n",
    "housing_prepared.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and Train a Model\n",
    "Finally we're at the actual modelling stage! We'll start with linear regression, then try a decision tree. No use getting fancy unless we know the simple things don't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = make_pipeline(preprocessing, LinearRegression())\n",
    "lin_reg.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `predict` method on the **training** set to see how well our model does. This is kind of a sanity check - if our performance is bad on the training set, it certainly won't be good on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The book just inspects a few values, but I prefer visualizing\n",
    "def plot_predictions(actual, pred):\n",
    "    plt.scatter(actual, pred, alpha=0.1)\n",
    "    plt.plot([0, 500000], [0, 500000], color=\"red\")\n",
    "    plt.xlabel(\"Actual Median House Value ($)\")\n",
    "    plt.ylabel(\"Predicted Median House Value ($)\")\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing)\n",
    "plot_predictions(housing_labels, housing_predictions)\n",
    "plt.title(\"Linear Regression Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
    "                              squared=False)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that wasn't great. Let's move on to a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn makes it pretty easy to swap out a model in the same pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\n",
    "tree_reg.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = tree_reg.predict(housing)\n",
    "\n",
    "plot_predictions(housing_labels, housing_predictions)\n",
    "plt.title(\"Decision Tree Prediction\")\n",
    "\n",
    "tree_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
    "                              squared=False)\n",
    "tree_rmse # wtf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**❓ Discussions for class:**\n",
    "- What is happening here?\n",
    "- What is under and over fitting?\n",
    "\n",
    "It's tempting to look at the performance on the test set at this point, but this can actually influence your choice of model! Best to keep the test set locked away until you're satisfied with the model. Rest assured that an RMSE of 0.0 is not a good sign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Evaluation Using Cross-Validation\n",
    "Rather than just a train/test split, we can add a third set called **validation**. This is a subset of the training set that is used to evaluate the model during training rather than to train parameters.\n",
    "\n",
    "**Cross-validation** is a technique where you split the training set $k$ times into a training/validation set, then train the model $k$ times. For example, if $k = 10$ you split the training set into 10 **non-overlapping** folds, pick one of them as the validation set and train on the other 9, then evaluate on the first. Repeat 10 times and you get 10 different models. The final model is the average of the 10 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "cv_predictions = cross_val_predict(tree_reg, housing, housing_labels, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(housing_labels, cv_predictions)\n",
    "plt.title(\"Decision Tree 10-fold Cross-Validated Prediction\")\n",
    "cv_mse = mean_squared_error(housing_labels, cv_predictions, squared=False)\n",
    "cv_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try one more model: random forest. This is an **ensemble** of decision trees, where each one is trained on a random subset of the training data and a random subset of the **features**. The final prediction is the average of the predictions of all the trees.\n",
    "\n",
    "**Warning:** the following cell may take a few minutes to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = make_pipeline(preprocessing,\n",
    "                           RandomForestRegressor(random_state=42))\n",
    "forest_reg.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_predictions = forest_reg.predict(housing)\n",
    "plot_predictions(housing_labels, forest_predictions)\n",
    "plt.title(\"Random Forest Prediction\")\n",
    "forest_rmse = mean_squared_error(housing_labels, forest_predictions, squared=False)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... not quite 0, but looks a bit too good to be true. Let's try measuring the cross-validation score (this is going to take a while, as it's training 10 random forest models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_forest_score = -cross_val_score(forest_reg, housing, housing_labels, \n",
    "                                   scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "cv_forest_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error is much lower than the validation error, which usually means that the model has overfit the training set. Another possible explanation may be that there's a mismatch between the training data and the validation data, but it's not the case here, since both came from the same dataset that we shuffled and split in two parts.\n",
    "\n",
    "**❓ Discussions for class:**\n",
    "- What are some ways to reduce overfitting?\n",
    "- What other models could we try for this task?\n",
    "- Is there anything in our preprocessing pipeline that we could change?\n",
    "\n",
    "Ultimately: garbage in, garbage out. Data quality is paramount!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune Your Model\n",
    "Let's assume that we tried a bunch of things and are forging ahead with the random forest model. We *could* tweak our hyperparameters (parameters about the model rather than model parameters) manually, but we can also be systematic about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "One option is to manually define the combinations of hyperparameters that we want to try, then train and evaluate the model on each combination. This is called **grid search**. It's straightforward, but expensive.\n",
    "\n",
    "**Warning:** the following cell may take a few minutes to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    (\"random_forest\", RandomForestRegressor(random_state=42)),\n",
    "])\n",
    "# Note: the original version of this notebook and the book reference the\n",
    "# geo clustering parameters that we skipped.\n",
    "param_grid = [\n",
    "    {'random_forest__max_features': [4, 6, 8]},\n",
    "]\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,\n",
    "                           scoring='neg_root_mean_squared_error')\n",
    "grid_search.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the full list of hyperparameters available for tuning by looking at `full_pipeline.get_params().keys()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – shows part of the output of get_params().keys()\n",
    "print(str(full_pipeline.get_params().keys())[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyperparameter combination found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the score of each hyperparameter combination tested during the grid search: (not really a combination now that I've reduced it to one hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
    "cv_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the final model behaviour\n",
    "grid_search_predictions = grid_search.best_estimator_.predict(housing)\n",
    "plot_predictions(housing_labels, grid_search_predictions)\n",
    "plt.title(\"Grid Search Random Forest Prediction\")\n",
    "grid_search_rmse = mean_squared_error(housing_labels, grid_search_predictions, squared=False)\n",
    "grid_search_rmse # probably still overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_  # includes preprocessing\n",
    "plt.bar(final_model[\"preprocessing\"].get_feature_names_out(), final_model[\"random_forest\"].feature_importances_)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model evaluation\n",
    "In the end, the grid search didn't help all that much, probably because it wasn't a true grid. The book also goes over some **randomized search** methods that are more efficient than grid search, but we'll skip that for now. There's also `HalvingRandomSearchCV` and `HalvingGridSearchCV` which do some more intelligent searching, but you could spend forever on this.\n",
    "\n",
    "**❓ Discussions for class:**\n",
    "- Which features were the most important?\n",
    "- Could we drop or combine any features to simplify the model?\n",
    "- Are there any other models we could try?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Your System on the Test Set\n",
    "We'll assume we're happy with the final model and want to deploy it. The last step is to evaluate it on the test set to get an estimate of the **generalization error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "grid_search_predictions = final_model.predict(X_test)\n",
    "\n",
    "final_rmse = mean_squared_error(y_test, grid_search_predictions, squared=False)\n",
    "plot_predictions(y_test, grid_search_predictions)\n",
    "plt.title(\"Test Set Prediction\")\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a higher number than in the book, but we skipped a few steps. However, we have to be careful with interpreting a single number - much better to look at the **confidence interval**.\n",
    "\n",
    "**🧮 Math break!**\n",
    "A [confidence interval](https://en.wikipedia.org/wiki/Confidence_interval) defines a range of values that tend towards a given confidence of containing the value. Assuming that the values are normally distributed (thanks CLT), we can [compute the confidence interval](https://en.wikipedia.org/wiki/Normal_distribution#Confidence_intervals) as:\n",
    "\n",
    "$$\\left[ \\hat\\mu - t_{n-1,1-\\alpha/2} \\frac{1}{\\sqrt{n}}s,\n",
    "                      \\hat\\mu + t_{n-1,1-\\alpha/2} \\frac{1}{\\sqrt{n}}s \\right]$$\n",
    "\n",
    "where $\\hat\\mu$ is the sample mean, $t_{n-1,\\alpha/2}$ is the $t$-score for confidence $1 - \\alpha$, $s$ is the sample standard deviation, and $n$ is the number of samples. $n-1$ is referred to as the **degrees of freedom**, and represents the logical maximum number of independent values that can be chosen freely.\n",
    "\n",
    "Back in the day we needed to use $t$-tables and interpolation, but now it's pretty easy to compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "confidence = 0.95\n",
    "squared_errors = (grid_search_predictions - y_test) ** 2\n",
    "n = len(squared_errors)\n",
    "np.sqrt(stats.t.interval(confidence, n - 1,\n",
    "                         loc=squared_errors.mean(),\n",
    "                         scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also do it manually like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – shows how to compute a confidence interval for the RMSE\n",
    "mean = squared_errors.mean()\n",
    "tscore = stats.t.ppf((1 - confidence) / 2, df=n - 1)\n",
    "tmargin = tscore * squared_errors.std(ddof=1) / np.sqrt(n)\n",
    "np.sqrt(mean + tmargin), np.sqrt(mean - tmargin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use a z-score rather than a t-score if the dataset is big enough. This assumes that the estimate of the standard deviation is representative of the population, which is not always the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – computes a confidence interval again using a z-score\n",
    "zscore = stats.norm.ppf((1 - confidence) / 2)\n",
    "zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(n)\n",
    "np.sqrt(mean + zmargin), np.sqrt(mean - zmargin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model persistence using joblib\n",
    "After all this, you want a useful model that you can deploy in production. The last step is to save all those parameters you just trained so that you can load them later. Pickle could be used for this, but `joblib` is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(final_model, \"my_california_housing_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can deploy this model to production. For example, the following code could be a script that would run in production:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# extra code – excluded for conciseness\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "def column_ratio(X):\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "#class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "#    [...]\n",
    "\n",
    "final_model_reloaded = joblib.load(\"my_california_housing_model.pkl\")\n",
    "\n",
    "new_data = housing.iloc[:5]  # pretend these are new districts\n",
    "predictions = final_model_reloaded.predict(new_data)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's the basic process! The rest of this course will dig in to the details of each step and look at a number of different ways of representing data, different models, and some of the math hiding behind it all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
