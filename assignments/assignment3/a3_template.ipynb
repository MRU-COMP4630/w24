{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Natural Language Processing and Classification\n",
    "Name(s):\n",
    "\n",
    "Note: the report with the following headings can be completed in this notebook, or in a separate document.\n",
    "\n",
    "## 1. Dataset Description\n",
    "A description of the dataset, including:\n",
    "1. Biases and limitations of the dataset\n",
    "2. Class (im)balance\n",
    "3. A summary of your impressions of the dataset\n",
    "\n",
    "## 2. Documentation of Experiments\n",
    "A description of the various things you tried, including:\n",
    "1. Changes to the vectorization/tokenization/other preprocessing\n",
    "2. Changes to the model architecture\n",
    "3. Changes to hyperparameters, loss functions, and optimizers\n",
    "4. A brief summary of things you learned in the process\n",
    "\n",
    "Some of this might be best presented in a table or list format.\n",
    "\n",
    "## 3. Your Final Model\n",
    "A description of the final \"best\" model you built, including:\n",
    "1. The preprocessing pipeline\n",
    "2. The architecture and model hyperparameters\n",
    "3. A brief discussion/your best guess as to why this model outperformed the others\n",
    "\n",
    "## 4. Discussion/Conclusion\n",
    "A discussion/conclusion section describing:\n",
    "1. Challenges, advantages, and limitations of the process or model\n",
    "2. Your key takeaways from the process\n",
    "3. What you would do if you had more time\n",
    "4. Any other thoughts or observations you have about NLP and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Code\n",
    "Here's some BigQuery related stuff and a terrible network to get you started. Feel free to change just about anything in this notebook.\n",
    "\n",
    "## Load the BigQuery data\n",
    "I highly recommend using [Colab](https://colab.research.google.com/) for this assignment, as it involves some hefty computation. If you have a GPU that you've managed to convince Tensorflow to work with, you can also use the [BigQuery Python client library](https://cloud.google.com/bigquery/docs/reference/libraries#client-libraries-install-python) to load the data. \n",
    "\n",
    "The following cell assumes you are using Colab and will prompt you for your Google credentials. You may need to activate the BigQuery API for your account first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "print('Authenticated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with the BigQuery API, do the following:\n",
    "1. Create a project in the [Google Cloud Console](https://console.cloud.google.com/). The free tier provides up to 1TB of querying per month, so you should be fine, but you can also claim the free credit provided for this course (see instructions on D2L under \"Course Info\").\n",
    "2. Enable the BigQuery API for your project.\n",
    "3. Change the `\"<your project id here>\"` placeholder in the next cell to the project ID from your Google Cloud Console (probably a slugified version of your project name).\n",
    "\n",
    "Here I'm loading 100k random rows from the [Stack Overflow dataset](https://console.cloud.google.com/marketplace/details/stack-exchange/stack-overflow), and limiting the selection to questions with a single tag taken from the top 10 most common tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"<your project id here>\"\n",
    "\n",
    "%%bigquery df --project $project_id\n",
    "SELECT title, tags\n",
    "FROM `bigquery-public-data.stackoverflow.posts_questions` as Posts\n",
    "WHERE ARRAY_LENGTH(SPLIT(Posts.tags, '|')) = 1 AND tags in (\n",
    "  SELECT tags\n",
    "  FROM `bigquery-public-data.stackoverflow.posts_questions` as Posts\n",
    "  WHERE ARRAY_LENGTH(SPLIT(Posts.tags, '|')) = 1\n",
    "  GROUP BY tags\n",
    "  ORDER BY COUNT(*) DESC\n",
    "  LIMIT 10\n",
    ")\n",
    "ORDER BY RAND()\n",
    "LIMIT 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the distribution of tags\n",
    "df['tags'].hist()\n",
    "\n",
    "# Look at the first five rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to do some class balancing or some other preprocessing, but you'll definitely need to do the following:\n",
    "1. Split the data into training/validation/testing sets\n",
    "2. Encode the labels as integers or one-hot vectors (there's lots of ways - I chose to use Pandas' `get_dummies` function)\n",
    "3. Define a `TextVectorization` layer to normalize, split, and map strings to integers - don't forget to take care of the padding values, either by masking or using `RaggedTensor`s\n",
    "4. Define some kind of model, hopefully better than the one I use as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tv_thresh = int(.7 * len(df))\n",
    "vt_thresh = int(.85 * len(df))\n",
    "\n",
    "# train/val/test split (e.g. 70/15/15)\n",
    "train_X = df[\"title\"][:tv_thresh].values\n",
    "val_X = df[\"title\"][tv_thresh:vt_thresh].values\n",
    "test_X = df[\"title\"][vt_thresh:].values\n",
    "\n",
    "# encode the output labels\n",
    "labels = pd.get_dummies(df[\"tags\"])\n",
    "label_names = labels.columns\n",
    "train_y = labels[:tv_thresh].values\n",
    "val_y = labels[tv_thresh:vt_thresh].values\n",
    "test_y = labels[vt_thresh:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines the vectorization layer. Lots of important decisions to be made here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define the tokenization/sequencing parameters\n",
    "vocab_size = 2000\n",
    "vec_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    standardize=\"lower\",\n",
    "    ragged=True\n",
    ")\n",
    "vec_layer.adapt(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally here's a sample model that actually managed to get worse as training progressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    vec_layer,\n",
    "    tf.keras.layers.Embedding(vocab_size, 128),\n",
    "    tf.keras.layers.GRU(128),\n",
    "    tf.keras.layers.Dense(len(label_names))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x=train_X, \n",
    "    y=train_y,\n",
    "    validation_data=(val_X, val_y),\n",
    "    epochs=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
