{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)  # to make this code example reproducible\n",
    "m = 100  # number of instances\n",
    "x = 2 * np.random.rand(m, 1)  # column vector\n",
    "y = 4 + 3 * x + np.random.randn(m, 1)  # column vector\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"noisy_line_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mu_x = x.sum() / m\n",
    "mu_y = y.sum() / m\n",
    "\n",
    "theta_1 = (mu_y * x.sum() - (y * x).sum()) / (mu_x * x.sum() - (x * x).sum())\n",
    "theta_0 = mu_y - theta_1 * mu_x\n",
    "print(theta_0, theta_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(x, theta_0 + theta_1 * x, \"r-\", label=\"Best fit line\")\n",
    "plt.plot(x, 4 + 3 * x, \"g-\", label=\"True line\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.savefig(\"best_fit_line.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix implementation\n",
    "X = np.hstack([np.ones((m, 1)), x])  # add x0 = 1 to each instance\n",
    "\n",
    "theta_best = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(theta_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various functions\n",
    "def gradient(X, y, theta):\n",
    "    return 2 / X.shape[0] * X.T @ (X @ theta - y)\n",
    "\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent: Batch version\n",
    "np.random.seed(42)\n",
    "def gradient(X, y, theta):\n",
    "    return 2 / X.shape[0] * X.T @ (X @ theta - y)\n",
    "\n",
    "eta = 0.1  # learning rate\n",
    "n_iterations = 100\n",
    "theta_batch = np.zeros((n_iterations, 2))\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    theta = theta - eta * gradient(X, y, theta)\n",
    "    theta_batch[i] = theta.T\n",
    "\n",
    "batch_best = theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "theta_sgd = np.zeros((n_epochs * m, 2))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X[random_index : random_index + 1]\n",
    "        yi = y[random_index : random_index + 1]\n",
    "        gradients = gradient(xi, yi, theta)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_sgd[epoch * m + i] = theta.T\n",
    "\n",
    "sgd_best = theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch Gradient Descent\n",
    "n_epochs = 50\n",
    "minibatch_size = 20\n",
    "n_batches_per_epoch = m // minibatch_size\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "theta_mgd = np.zeros((n_epochs * n_batches_per_epoch, 2))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(n_batches_per_epoch):\n",
    "        xi = X_shuffled[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "        yi = y_shuffled[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "        gradients = gradient(xi, yi, theta)\n",
    "        eta = learning_schedule(epoch * n_batches_per_epoch + i)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_mgd[epoch * n_batches_per_epoch + i] = theta.T\n",
    "\n",
    "mgd_best = theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(theta_batch[:, 0], theta_batch[:, 1], \"b.-\", label=\"Batch gradient descent\")\n",
    "plt.plot(theta_sgd[:, 0], theta_sgd[:, 1], \"r.-\", label=\"Stochastic gradient descent\")\n",
    "plt.plot(theta_mgd[:, 0], theta_mgd[:, 1], \"g.-\", label=\"Mini-batch gradient descent\")\n",
    "plt.xlabel(r\"$\\theta_0$\")\n",
    "plt.ylabel(r\"$\\theta_1$\")\n",
    "plt.scatter(batch_best[0], batch_best[1], color='k', marker='x', label='Batch Final value', zorder=10)\n",
    "plt.scatter(sgd_best[0], sgd_best[1], edgecolors='k', facecolors='none', marker='o', label='SGD Final value', zorder=10)\n",
    "plt.scatter(mgd_best[0], mgd_best[1], edgecolors='k', facecolors='none', marker='s', label='Mini-batch Final value', zorder=10)\n",
    "plt.scatter(theta_best[0], theta_best[1], color='k', marker='*', label='Analytic value', zorder=10, s=100)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.axis([2.6, 4.6, 2.3, 3.4])\n",
    "plt.savefig(\"gradient_descent_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration of early stopping\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# extra code – creates the same quadratic dataset as earlier and splits it\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\n",
    "X_train, y_train = X[: m // 2], y[: m // 2, 0]\n",
    "X_valid, y_valid = X[m // 2 :], y[m // 2 :, 0]\n",
    "\n",
    "preprocessing =Pipeline([\n",
    "    (\"Poly Features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "     (\"Scaler\", StandardScaler())\n",
    "     ])\n",
    "\n",
    "\n",
    "X_train_prep = preprocessing.fit_transform(X_train)\n",
    "X_valid_prep = preprocessing.transform(X_valid)\n",
    "sgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)\n",
    "n_epochs = 500\n",
    "best_valid_rmse = float('inf')\n",
    "train_errors, val_errors = [], []  # extra code – it's for the figure below\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    sgd_reg.partial_fit(X_train_prep, y_train)\n",
    "    y_valid_predict = sgd_reg.predict(X_valid_prep)\n",
    "    val_error = mean_squared_error(y_valid, y_valid_predict, squared=False)\n",
    "    if val_error < best_valid_rmse:\n",
    "        best_valid_rmse = val_error\n",
    "        best_model = deepcopy(sgd_reg)\n",
    "\n",
    "    # extra code – we evaluate the train error and save it for the figure\n",
    "    y_train_predict = sgd_reg.predict(X_train_prep)\n",
    "    train_error = mean_squared_error(y_train, y_train_predict, squared=False)\n",
    "    val_errors.append(val_error)\n",
    "    train_errors.append(train_error)\n",
    "\n",
    "# extra code – this section generates and saves Figure 4–20\n",
    "best_epoch = np.argmin(val_errors)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.annotate('Best model',\n",
    "             xy=(best_epoch, best_valid_rmse),\n",
    "             xytext=(best_epoch, best_valid_rmse + 0.5),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "plt.plot([0, n_epochs], [best_valid_rmse, best_valid_rmse], \"k:\", linewidth=2)\n",
    "plt.plot(val_errors, \"b-\", linewidth=3, label=\"Validation set\")\n",
    "plt.plot(best_epoch, best_valid_rmse, \"bo\")\n",
    "plt.plot(train_errors, \"r--\", linewidth=2, label=\"Training set\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.axis([0, n_epochs, 0, 3.5])\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(\"early_stopping.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
