# Questions from this week
## Linear Regression
- What is the normal equation?
- What does it mean for a function to be convex?
- In broad steps, where does the normal equation come from? (Not the full derivation with all the algebra, but steps like "Set the gradient to zero and solve for the weights")
- What is the gradient?
- What is regularization?

## Gradient Descent
- What is the difference between the normal equation and gradient descent?
- What are stochastic, batch, and minibatch gradient descent?
- Why would you choose one over the other?
- Why is data normalization important for gradient descent?
- What is the learning rate or step size?
- How would you choose when to stop training?
- What is the role of the validation set?
- What criteria does a loss function need to meet to be suitable for gradient descent?
