# Questions for Week 10
Note: This was really week 10/11 as we only really covered up to attention mechanisms in week 10. We looked at "Attention is All You Need" in journal club.

## Sequence to sequence models
- What are some challenges in sequence to sequence modelling?
- How does a bidirectional RNN help in language modelling?
- How does a bidirectional RNN work?
- What is an encoder-decoder model?
- What is the context vector in an encoder-decoder model?
- How are language models trained?
- What are some popular tokenization strategies?
- What is teacher forcing?

## Attention mechanisms
- What is the motivation for attention mechanisms?
- How do attention mechanisms fit in with encoder-decoder models?
- What is the alignment model in an attention mechanism?
- What kinds of attention mechanisms are there, and which is currently the most popular?
- How can attention help with explainability?

## Transformers
- What was the motivation for the development of transformers?
- What are some key components of transformers?
- What is multi-head attention?
- What is positional encoding?
- What are the query, key, and value matrices in attention mechanisms?
- What transformer implementations are available (open or closed source)?
- What is the Hugging Face API?